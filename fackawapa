2010-04-02

ok so both stoagecollection and fileinfocollection should ideally inherit from same FileCollection superclass.
    - not so.
    - we just want to be able to compare the two's collections. anything else is dance theory.

move hide logger behind fileinfocollection.

WorkingCollection


we can compare two states of history..
    usually the current state (recently built from history) or the state as we see it on disk..
        - we dont really compare the state by each fileinfo with some other checks
        - FileInfoCollection readStateFromDisk()
        - compare disk_state to history_state?
          - history_state can compare itself to disk_state and generate log_events detailing the operations necessary to be identical to disk_state.
          - a set of log events returned


          and you can take em or leave em..

this is quite different from the process of requesting files
    we can ask if a collection has a file (by name or key)

a fileinfo collection has
    filepaths, modified dates
    sometimes has copies of files mentioned.
    has a close relationship with a on disk representation

FileInfoCollection is at one state.

should try to get disk state seperate from history state until comparison.

    a history log
        state
            modify state by adding file operations (log entries)


        logger
            is all about reading and writing stored state/history

    which can be accessed directly via filePath or through collection with key


a storagecollection
    key access to objects, these are not fileinfos..
    does not know filepaths or modified dates or history
    just objects, their size, and their fingerprint.
    is dumb.. and should not have much logic on it.. i think
    actually has all the data it lists.
    you can add and remove objects with a key. the storage is abstracted

2010-04-01
seperate the scan and the result detection so that we know what files are missing before we mark them as deleted..
this is necessary for the stoagecollection to know what to introduce to the working directory.
having online historys will be good.

some other time

am sure that using zsync in an unintrusive way to distribute updates to files would be sweet.

backend stores
hashbuckets
local file storage
amazon storage
limited size local backends
ignore command
revert command
include command?
branching? yes.. had already thought of this.. .. ways to search other peoples fileinfocollections
bittorrent distribution (easy to enable in s3)
client to client? just bitorrent again? dont waste time thinking about it yet
gui interface
options for hashbuckets:
    dudley storage formats
            amazon
                stores file in amazon, using their hash as the key
                uses a .dudley/reponame/ prefix on the key?
                maxSize
                computes upload cost and storage cost

            hash_dir
                next thing i will build.
                copies files specified by a fileinfocollection into a hash_dir
                stores files and allows preservation of history
                maxSize
                path

            working_dir
                no going to persue
                just have .dudley/history/timestamp.logs
                two computers could accept eachothers changes if they wanted
                    how would the dudley clients talk to eachother..via amazon initially? so both talking to hashstore
                    later on they could communicate directly with tcp
                stores files by filePath on disk
                no history just accept and push changes

    amazon backing store


hash_dir
StorageCollection
    // keys are supposed to be sha1 strings
    bool hasFile(key)
    bool addFile(key, QFile *file) (key, value)
    QFile* getFile(key)

    // given a pointer to a fileinfocollection.. something tell us what you got..
    QStringList matchFingerPrints(QStringList *prints)

distribution of histories
    a .dudley file eg robsphotos.dudley specifies history files to play to get a fileinfocollection, they are kept relative to the .dudley file
        ideally we will beable to compress .gz these and we can merge .log files
        .dudley/history/2009-231244532535.log
        .dudley/history/2010-242342353252.log

        lists history files you must play inorder to view the current filesystem, and know the keys for files in the hashbuckets mentioned.

        points to a head of history http://robguthrie.com/photos.duds

mikes photos.dudley - dudley info file to enable you to pull history files from some http thing..
    hashbucket path www.mikeszomzom.com/photos/
        from where you can grab
            .dudley/history/*.log - pieces of history
            .dudley/branches/master

